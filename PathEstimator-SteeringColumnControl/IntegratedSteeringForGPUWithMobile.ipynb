{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The given script captures a live feed from an android smartphone camera, estimates the path in accordance to the cone placement\n",
    "#and then sends the appropriate steering angle to an arduino which controls an actuator attached to the steering column.\n",
    "#(Runs on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial #Serial imported for Serial communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-44db28abe01b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "#from matplotlib import pyplot as plt\n",
    "import logging\n",
    "import logging.config\n",
    "import time\n",
    "import serial #Serial imported for Serial communication\n",
    "#from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from utils import cv_utils\n",
    "from utils import operations as ops\n",
    "from utils import tf_utils\n",
    "URL = \"http://192.168.43.1:8080/video\"\n",
    "ArduinoSerial = serial.Serial('COM7',9600)\n",
    "time.sleep(2)\n",
    "logging.config.fileConfig('logging.ini')\n",
    "\n",
    "VIDEO_PATH = 'sample_video.mp4'\n",
    "FROZEN_GRAPH_PATH = 'models/ssd_mobilenet_v1/frozen_inference_graph.pb'\n",
    "\n",
    "OUTPUT_WINDOW_WIDTH = None  # Use None to use the original size of the image\n",
    "DETECT_EVERY_N_SECONDS = None  # Use None to perform detection for each frame\n",
    "\n",
    "# TUNE ME\n",
    "CROP_WIDTH = CROP_HEIGHT = 600\n",
    "CROP_STEP_HORIZONTAL = CROP_STEP_VERTICAL = 600 - 20  # no cone bigger than 20px\n",
    "SCORE_THRESHOLD = 0.2\n",
    "NON_MAX_SUPPRESSION_THRESHOLD = 0.3\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read TensorFlow graph\n",
    "    detection_graph = tf_utils.load_model(FROZEN_GRAPH_PATH)\n",
    "\n",
    "    # Read video from disk and count frames\n",
    "    cap = cv2.VideoCapture(URL)\n",
    "    #cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    #CROP_WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    #CROP_HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
    "        #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        #out = cv2.VideoWriter('Prototype.avi', fourcc, 5, (int(cap.get(3)), int(cap.get(4))))\n",
    "        processed_images = 0\n",
    "        while cap.isOpened():\n",
    "            width = cap.get(3)\n",
    "            height = cap.get(4)\n",
    "            fps = cap.get(5)\n",
    "            if DETECT_EVERY_N_SECONDS:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES,\n",
    "                        processed_images * fps * DETECT_EVERY_N_SECONDS)\n",
    "            ##size = (width, height)\n",
    "            ##img_resp=requests.get(URL)\n",
    "            ##img_arr = np.array(bytearray(img_resp.content),dtype=np.uint8)\n",
    "            ##frame = cv2.imdecode(img_arr,-1)\n",
    "            ##height,width,channel=frame.shape\n",
    "            ret, frame = cap.read()\n",
    "            #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            if ret:\n",
    "                tic = time.time()\n",
    "                i=0\n",
    "                cv2.line(frame, (int(width/2),int(height/2)), (int(width/2),int(height/2)+40), (255,0,0), 10)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                \n",
    "                # crops are images as ndarrays of shape\n",
    "                # (number_crops, CROP_HEIGHT, CROP_WIDTH, 3)\n",
    "                # crop coordinates are the ymin, xmin, ymax, xmax coordinates in\n",
    "                #  the original image\n",
    "                crops, crops_coordinates = ops.extract_crops(\n",
    "                    frame, CROP_HEIGHT, CROP_WIDTH,\n",
    "                    CROP_STEP_VERTICAL, CROP_STEP_VERTICAL)\n",
    "\n",
    "                # Uncomment this if you also uncommented the two lines before\n",
    "                #  creating the TF session.\n",
    "                # crops = np.array([crops[0]])\n",
    "                # crops_coordinates = [crops_coordinates[0]]\n",
    "\n",
    "                detection_dict = tf_utils.run_inference_for_batch(crops, sess)\n",
    "\n",
    "                # The detection boxes obtained are relative to each crop. Get\n",
    "                # boxes relative to the original image\n",
    "                # IMPORTANT! The boxes coordinates are in the following order:\n",
    "                # (ymin, xmin, ymax, xmax)\n",
    "                boxes = []\n",
    "                for box_absolute, boxes_relative in zip(\n",
    "                        crops_coordinates, detection_dict['detection_boxes']):\n",
    "                    boxes.extend(ops.get_absolute_boxes(\n",
    "                        box_absolute,\n",
    "                        boxes_relative[np.any(boxes_relative, axis=1)]))\n",
    "                if boxes:\n",
    "                    boxes = np.vstack(boxes)\n",
    "\n",
    "                # Remove overlapping boxes\n",
    "                boxes = ops.non_max_suppression_fast(\n",
    "                    boxes, NON_MAX_SUPPRESSION_THRESHOLD)\n",
    "\n",
    "                # Get scores to display them on top of each detection\n",
    "                boxes_scores = detection_dict['detection_scores']\n",
    "                boxes_scores = boxes_scores[np.nonzero(boxes_scores)]\n",
    "\n",
    "                for box, score in zip(boxes, boxes_scores):\n",
    "                    if score > SCORE_THRESHOLD:\n",
    "                        i=i+1\n",
    "                        color = (0, 255, 0) \n",
    "                        thickness=10\n",
    "                        ymin, xmin, ymax, xmax = box\n",
    "                        if i==1:\n",
    "                            x1=(xmin+xmax)/2\n",
    "                            y1=(ymin+ymax)/2\n",
    "                        elif i==2:\n",
    "                            x2=(xmin+xmax)/2\n",
    "                            y2=(ymin+ymax)/2\n",
    "                            start_point=(int((x1+x2)/2),int(y1))\n",
    "                            end_point=(int((x1+x2)/2),int(y1)+40)\n",
    "                            if (x1>(width/2) and x2<(width/2)) or (x1<(width/2) and x2>(width/2)):\n",
    "                                cv2.line(frame, start_point, end_point, color, thickness)\n",
    "                                diff=(width/2)-((x1+x2)/2)+160\n",
    "                                cv2.putText(frame,str(diff),(int(width/2),int(height)-600), font, 3,(255,255,255),2,cv2.LINE_AA)\n",
    "                                diff_2=str(diff)\n",
    "                                ArduinoSerial.write(diff_2.encode())\n",
    "                                print (ArduinoSerial.readline())\n",
    "                        color_detected_rgb = cv_utils.predominant_rgb_color(\n",
    "                            frame, ymin, xmin, ymax, xmax)\n",
    "                        text = '{:.2f}'.format(score)\n",
    "                        cv_utils.add_rectangle_with_text(\n",
    "                            frame, ymin, xmin, ymax, xmax,\n",
    "                            color_detected_rgb, str(i))\n",
    "\n",
    "                if OUTPUT_WINDOW_WIDTH:\n",
    "                    frame = cv_utils.resize_width_keeping_aspect_ratio(\n",
    "                        frame, OUTPUT_WINDOW_WIDTH)\n",
    "\n",
    "                \n",
    "                cv2.waitKey(1)\n",
    "                processed_images += 1\n",
    "\n",
    "                toc = time.time()\n",
    "                processing_time_ms = (toc - tic) * 1000\n",
    "                logging.debug(\n",
    "                    'Detected {} objects in {} images in {:.2f} ms'.format(\n",
    "                        len(boxes), len(crops), processing_time_ms))\n",
    "                cv2.imshow('Live Video', frame)\n",
    "                #out.write(frame)\n",
    "                #out = cv2.VideoWriter('test_file.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (width,height))\n",
    "                #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    #break\n",
    "            else:\n",
    "                # No more frames. Break the loop\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
